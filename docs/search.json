[
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#roadmap-for-today",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#roadmap-for-today",
    "title": "Final Exam Review",
    "section": " Roadmap for Today",
    "text": "Roadmap for Today\n\nGo through some slides (including some interactive problems)\nWork through some problems together (on the worksheet; copies can be found at the front of the room)\n\n\n\n\n\n\n\n\nDisclaimer\n\n\nI have not seen the exam yet, so I do not know exactly what will or will not be on it. Just because something does or does not show up on these slides doesn’t mean it is guaranteed to show up / not show up on the exam.\n\n\n\n\n\n\n\n\n\n\n\nDisclaimer\n\n\nThis review is not intended to be comprehensive; I encourage you to consult the lecture notes, textbook, homework, and your own notes."
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#roadmap-for-today-1",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#roadmap-for-today-1",
    "title": "Final Exam Review",
    "section": " Roadmap for Today",
    "text": "Roadmap for Today\n\nNow, there is far too much material for me to be able to meaningfully cover everything that I think is important for the final.\nInstead, I’ve elected to select a handful of topics which I think might be confusing (or topics I’d like to expound upon). I’ll go through these relatively quickly, though, as the best way to learn is to practice - so I’d like to leave plenty of time for us to work through some of the problems on the worksheet!\nOrder of coverage:\n\nEstimation\nCI for a Difference in Means (time permitting)\nHypothesis Testing"
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#estimation-1",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#estimation-1",
    "title": "Final Exam Review",
    "section": " Estimation",
    "text": "Estimation\nGeneral Framework\n\n\n\nWe have a population, governed by a set of population parameters that are unobserved (but that we’d like to make claims about).\nTo make claims about the population parameters, we take a sample.\nWe then use our sample to make inferences (i.e. claims) about the population parameters.\n\n\n\n\n\n\n\nInference can mean either estimation or hypothesis testing."
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#estimation-2",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#estimation-2",
    "title": "Final Exam Review",
    "section": " Estimation",
    "text": "Estimation\nTerminology\n\nIn estimation, we seek to estimate a particular population parameter.\nWe do so by taking a sample (Y1, …, Yn) from the population, and constructing an estimator: \\[ \\widehat{\\theta}_n := \\widehat{\\theta}_n(Y_1, \\cdots, Y_n) \\]\nCrucially, an estimator is a random quantity.\n\nContrast this with an estimate, which we obtain by plugging specific data into our estimator.\nE.g. we use the sample mean as an estimator for the population mean; after getting a specific set of observations, their numerical sample mean is the estimate."
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#estimation-3",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#estimation-3",
    "title": "Final Exam Review",
    "section": " Estimation",
    "text": "Estimation\nProperties\n\nA “good” point estimator is one that possesses one (or several) desirable properties, which we can measure in a few different ways:\n\nUnbiasedness: \\(\\E[\\widehat{\\theta}_n] = \\theta\\)\nConsistency: \\(\\widehat{\\theta}_n \\probto \\theta\\)\nMSE: \\(\\mathrm{MSE}(\\widehat{\\theta}_n) = \\mathrm{Bias}^2(\\widehat{\\theta}_n) + \\Var(\\widehat{\\theta}_n)\\)\n\nQuestion: do we want high or low MSE?\n\nMVUE: \\(\\widehat{\\theta}_n\\) is unbiased and possesses the smallest variance among all possible unbiased estimators.\n\nCheck your understanding: are all consistent estimators unbiased? Are all unbiased estimators consistent?"
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#consistency",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#consistency",
    "title": "Final Exam Review",
    "section": " Consistency",
    "text": "Consistency\nDefinition\n\n\n\n\n\n\nDefinition: Consistency\n\n\nAn estimator \\(\\widehat{\\theta}_n\\) is said to be a consistent estimator for \\(\\theta\\), denoted \\(\\widehat{\\theta}_n \\probto \\theta\\) if, for any \\(\\varepsilon &gt; 0\\), either of the following equivalent statements hold: \\[\\begin{align*}\n  \\lim_{n \\to \\infty} \\Prob\\left( |\\widehat{\\theta}_n - \\theta| \\leq \\varepsilon \\right)  & = \\underline{\\qquad \\qquad} \\\\\n  \\lim_{n \\to \\infty} \\Prob\\left( |\\widehat{\\theta}_n - \\theta| &gt;  \\varepsilon \\right)  & = \\underline{\\qquad \\qquad}\n\\end{align*}\\]\n\n\n\n\n\\(|\\widehat{\\theta}_n - \\theta| \\leq \\varepsilon\\) means “the distance between \\(\\widehat{\\theta}_n\\) and \\(\\theta\\) is very small.” Equivalently: “\\(\\widehat{\\theta}_n\\) is very close to \\(\\theta\\).”\nThe definition of consistency asserts that this probability goes to zero as the sample size increases. That is: “as our sample size becomes larger, we become more certain that \\(\\widehat{\\theta}_n\\) is very close to \\(\\theta\\).”"
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#consistency-1",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#consistency-1",
    "title": "Final Exam Review",
    "section": " Consistency",
    "text": "Consistency\nBiased but Consistent\nExample: \\(\\widehat{\\sigma^2}_n := \\frac{1}{n} \\sum_{i=1}^{n} (Y_i - \\bar{Y}_n)^2\\)"
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#estimation-4",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#estimation-4",
    "title": "Final Exam Review",
    "section": " Estimation",
    "text": "Estimation\nConstructing Estimators\n\nSo far, we’ve primarily been concerned with assessing the performance of an estimator. Now, we turn our attention to the question of how to construct an estimator.\nThere are two main methods we use:\n\nThe Method of Moments (MoM)\nThe method of Maximum Likelihood Estimation (MLE)\n\nIntuition behind the method of moments: our sample moments should closely match the population moments (the sample average cat weight should probably be close to the true average of all cat weights).\nIntuition behind maximum likelihood estimation: a good guess for the true value of the parameter is that was most likely to have given rise to the data we observed."
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#method-of-moments",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#method-of-moments",
    "title": "Final Exam Review",
    "section": " Method of Moments",
    "text": "Method of Moments\n\n\n\n\n\n\nMethod of Moments\n\n\n\nSet up p equations (where p is the number of parameters that are desired to be estimated) of the form \\[\\begin{align*}\n  M_1   &= \\mu_1 \\\\\n  M_2   & = \\mu_2  \\\\\n  \\vdots & \\hspace{5mm} \\vdots \\\\\n  M_p   & = \\mu_p\n\\end{align*}\\] where \\[ M_k := \\frac{1}{n} \\sum_{i=1}^{n} Y_i^k ; \\qquad \\mu_k := \\E[Y_i^k] \\] denote the kth sample moment and population moment, respectively\nSolve the equations for the p parameters; these will be the method of moments estimators for the parameters."
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#example-2",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#example-2",
    "title": "Final Exam Review",
    "section": " Example 2",
    "text": "Example 2\n\n\n\n\n\n\nExample 1\n\n\nLet \\(Y_1, \\cdots, Y_n \\iid \\mathrm{Geom}(p)\\). Derive an expression for \\(\\widehat{p}_{\\mathrm{MoM}}\\), the method of moments estimator for p.\n\n\n\n\nWe have only one parameter, so we only need to set up one equation.\nThe first population moment is given by \\(\\mu_1 := \\E[Y_i] = 1/p\\)\nHence, our method of moments estimator satisfies the equation \\[ \\overline{Y}_n = \\frac{1}{\\widehat{p}_{\\mathrm{MoM}}} \\]\nWhen solved for \\(\\widehat{p}_{\\mathrm{MoM}}\\), we obtain \\(\\boxed{\\widehat{p}_{\\mathrm{MoM}} = \\frac{1}{\\overline{Y}_n}}\\)"
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#sampling",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#sampling",
    "title": "Final Exam Review",
    "section": " Sampling",
    "text": "Sampling\nExample: Cats!"
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#leadup",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#leadup",
    "title": "Final Exam Review",
    "section": " Leadup",
    "text": "Leadup\n\nEach one of these samples provides some information about µ, the true average weight of all cats.\nFor example, suppose we only observed the first sample: \\[ \\vec{\\boldsymbol{y}} = (8.5, \\ 12.0, \\ 7.5, \\ 11.1, ... , \\ 8.8, 10.4) \\]\n\nFor reference, the average weight of cats in this sample is 9.11 lbs.\n\nGiven this sample, how likely do we think it is that the true average weight of all cats is, say, 8 lbs?\nGiven this sample, how likely do we think it is that the true average weight of all cats is, say, 30 lbs?\nGiven this sample, how likely do we think it is that the true average weight of all cats is, say, some arbitrary value µ?"
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#likelihoods",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#likelihoods",
    "title": "Final Exam Review",
    "section": " Likelihoods",
    "text": "Likelihoods\n\nThe answer to this last question is precisely the likelihood of a sample.\nMore generally, \\[ \\Lik(\\theta; Y_1, \\cdots, Y_n) \\] denotes the likelihood of the true value of the parameter being θ, given observations (Y1, …, Yn).\nMathematically, the likelihood is just the joint density function of (Y1, …, Yn); conceptually, we are now viewing it as a function of θ.\nAs a concrete example, suppose \\(Y_1, \\cdots, Y_n \\iid \\mathcal{N}(\\mu, 1)\\) (if it helps, you can think of these at cat weights)."
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#likelihoods-1",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#likelihoods-1",
    "title": "Final Exam Review",
    "section": " Likelihoods",
    "text": "Likelihoods\nExample\n\nSince our sample is stated to be i.i.d.,\n\n\\[\\begin{align*}\n  \\class{fragment}{{} \\Lik(\\theta; Y_1, \\cdots, Y_n) }\n    &\\class{fragment}{{} := f_{Y_1, \\cdots, Y_n}(y_1, \\cdots, y_n ; \\theta) }            \\\\[3px]\n    &\\class{fragment}{{}  = \\prod_{i=1}^{n} f_{Y_i}(y_i; \\theta) }    \\\\[3px]\n    &\\class{fragment}{{}  = \\prod_{i=1}^{n} \\left[ \\frac{1}{\\sqrt{2\\pi}} \\exp\\left\\{ - \\frac{1}{2} (Y_i - \\mu)^2 \\right\\} \\right]  }    \\\\[3px]\n    &\\class{fragment}{{}  = \\left( \\frac{1}{2\\pi} \\right)^{n/2} \\cdot \\exp\\left\\{ - \\frac{1}{2} \\sum_{i=1}^{n} (\\mu - Y_i)^2 \\right\\}  }\n\\end{align*}\\]"
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#likelihoods-2",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#likelihoods-2",
    "title": "Final Exam Review",
    "section": " Likelihoods",
    "text": "Likelihoods\nExample"
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#likelihoods-3",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#likelihoods-3",
    "title": "Final Exam Review",
    "section": " Likelihoods",
    "text": "Likelihoods\nExample"
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#maximum-likelihood-estimation",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#maximum-likelihood-estimation",
    "title": "Final Exam Review",
    "section": " Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation\n\nConsider again the likelihood of a sample; \\(\\Lik(\\theta; Y_1, \\cdots, Y_n)\\).\nRecall that this represents how likely any specified value of θ is to be the truth, given the data (Y1, …, Yn).\nA good guess for the true value of θ, therefore, is perhaps the one that was most likely to have given rise to the data we observed.\n\nIn other words, the value that maximizes the likelihood.\n\n\n\n\n\n\n\n\n\nDefinition: Maximum Likelihood Estimator\n\n\n\\[ \\widehat{\\theta}_{\\mathrm{MLE}} := \\argmax_{\\theta} \\left\\{ \\Lik(\\theta; Y_1, \\cdots, Y_n) \\right\\} \\]\n\n\n\n\n\nSometimes it’s more convenient to work with the log-likelihood, though it is not always necessary."
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#example-3-1",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#example-3-1",
    "title": "Final Exam Review",
    "section": " Example 3",
    "text": "Example 3\n\n\n\n\n\n\nExample 3\n\n\nLet \\(Y_1, \\cdots, Y_n \\iid \\mathcal{N}(\\mu, 1)\\). Derive an expression for \\(\\widehat{\\mu}_{\\mathrm{MLE}}\\), the maximum likelihood estimator for µ.\n\n\n\n\nFrom before, the likelihood is \\[ \\Lik(\\mu; Y_1, \\cdots, Y_n) = \\left( \\frac{1}{2\\pi} \\right)^{n/2} \\cdot \\exp\\left\\{ - \\frac{1}{2} \\sum_{i=1}^{n} (\\mu - Y_i)^2 \\right\\}\\]\nThe log-likelihood and its first derivative are therefore given by \\[\\begin{align*}\n  \\class{fragment}{{} \\ell(\\mu; Y_1, \\cdots, Y_n) }\n&\\class{fragment}{{} = -\\frac{n}{2} \\ln(2\\pi) - \\frac{1}{2} \\sum_{i=1}^{n}(Y_i - \\mu)^2 }            \\\\[3px]\n\\class{fragment}{{} \\frac{\\partial}{\\partial \\mu} \\ell(\\mu; Y_1, \\cdots, Y_n) } & \\class{fragment}{{}  = \\sum_{i=1}^{n} (Y_i - \\mu) = n \\bar{Y} - n \\mu }  \n\\end{align*}\\]"
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#example-2-1",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#example-2-1",
    "title": "Final Exam Review",
    "section": " Example 2",
    "text": "Example 2\n\n\n\n\n\n\nExample 2\n\n\nLet \\(Y_1, \\cdots, Y_n \\iid \\mathcal{N}(\\mu, 1)\\). Derive an expression for \\(\\widehat{\\mu}_{\\mathrm{MLE}}\\), the maximum likelihood estimator for µ.\n\n\n\n\nSetting this equal to zero and solving for µ reveals that a critical value of the likelihood is given by \\(\\mu = \\overline{Y}_n\\).\n\n\nThe second derivative of the log-likelihood is given by \\[\\frac{\\partial^2}{\\partial \\mu^2} \\ell(\\theta; Y_1, \\cdots, Y_n) = - n \\] which is negative everywhere; hence the critical value we found above must be a maximum.\nThus, \\(\\boxed{\\widehat{\\mu}_{\\mathrm{MLE}} = \\overline{Y}_n}\\)"
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#maximum-likelihood-estimation-1",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#maximum-likelihood-estimation-1",
    "title": "Final Exam Review",
    "section": " Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation\n\nIf the support of the population distribution depends on the parameter of interest, the likelihood will be nondifferentiable (with respect to the parameter of interest).\nIn such cases, the likelihood must be maximized by inspection - there’s an example of this on the worksheet we’ll go over later today.\n\n\n\n\n\n\n\n\nCaution\n\n\nIn cases like this (where the support depends on the parameter), do NOT forget about the indicator in the density function."
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#leadup-1",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#leadup-1",
    "title": "Final Exam Review",
    "section": " Leadup",
    "text": "Leadup\n\nDo UCSB students have, on average, the same commute times as SBCC students?\nAssume we have two samples: \\[\\begin{align*}\n  Y_{1,1}, Y_{1,2}, \\cdots, Y_{1, n_1} & \\iid \\mathcal{N}(\\mu_1, \\ \\sigma^2) \\\\\n  Y_{2,1}, Y_{2,2}, \\cdots, Y_{2, n_2} & \\iid \\mathcal{N}(\\mu_2, \\ \\sigma^2)\n\\end{align*}\\] (note crucially that we are assuming the two population variances are equal).\n\nFor example, the Y1,i might represent UCSB commute times and the Y2,i might represent SBCC commute times.\n\nSay we want to construct a confidence interval for (µ1 - µ2 ), the difference in true average commute times."
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#confidence-interval-for-a-difference-in-means",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#confidence-interval-for-a-difference-in-means",
    "title": "Final Exam Review",
    "section": " Confidence Interval for a Difference in Means",
    "text": "Confidence Interval for a Difference in Means\nBy previously-established results, \\[\\begin{align*}\n  \\bar{Y}_1 := \\frac{1}{n_1} \\sum_{i=1}^{n_1} Y_i & \\sim  \\\\\n  \\bar{Y}_2 := \\frac{1}{n_2} \\sum_{i=1}^{n_2} Y_i & \\sim\n\\end{align*}\\] which in turn implies \\[ (\\bar{Y}_1 - \\bar{Y}_2)  \\sim  \\] \\[ \\frac{(\\bar{Y}_1 - \\bar{Y}_2) - \\qquad \\qquad \\qquad }{} \\sim \\mathcal{N}(0, 1) \\]"
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#confidence-interval-for-a-difference-in-means-1",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#confidence-interval-for-a-difference-in-means-1",
    "title": "Final Exam Review",
    "section": " Confidence Interval for a Difference in Means",
    "text": "Confidence Interval for a Difference in Means\n\nIn practice, \\(\\sigma^2\\) is often unknown so we replace it with an unbiased estimator: the pooled sample variance \\[ S_p^2 := \\left( \\frac{n_1 - 1}{n_1 + n_2 - 2} \\right) S_1^2  + \\left( \\frac{n_2 - 1}{n_1 + n_2 - 2} \\right) S_2^2 \\]\n\nIntuition: we take a weighted average of the two sample standard deviations, placing more weight on the sample with more information (i.e. a greater sample size), that is still an unbiased estimator for \\(\\sigma^2\\).\n\nReplacing \\(\\sigma^2\\) with \\(S_p := \\sqrt{S_p^2}\\) breaks the normality of our point estimator, requiring us to instead use the \\(t_{n_1 + n_2 - 2}\\) distribution."
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#confidence-interval-for-a-difference-in-means-2",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#confidence-interval-for-a-difference-in-means-2",
    "title": "Final Exam Review",
    "section": " Confidence Interval for a Difference in Means",
    "text": "Confidence Interval for a Difference in Means\n\\[ (\\bar{Y}_1 - \\bar{Y}_2) \\pm t_{n_1 + n_2 - 2, \\ \\frac{\\alpha}{2}} \\cdot S_p \\cdot \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\\]\n\nAssumptions:\n\n\nNormally-distributed population\nEqual population variances\n\n\n\n\nMake sure you understand how to interpret these intervals with respect to whether or not zero is contained in them."
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#cats",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#cats",
    "title": "Final Exam Review",
    "section": " Cats",
    "text": "Cats\nToe Beans…\n\nAccording to a Quora post, the average cat has about a 10% chance of being born with polydactyly\n\n\n\n\n\n\n\nImage Source: https://www.treehugger.com/thing-didnt-know-polydactyl-cats-4864197\n\n\n\n\n\nPolydactyly refers to a condition whereby an animal is born with extra digits (e.g. extra fingers in humans, extra toes in cats, etc.)\nSuppose we wish to assess the validity of the Quora claim, using data.\n\nNote that we’re not necessarily trying to estimate the true incidence of polydactyly among cats!"
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#cats---again",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#cats---again",
    "title": "Final Exam Review",
    "section": " Cats - Again!",
    "text": "Cats - Again!\nToe Beans…\n\nSay we collect a simple random sample of 100 cats, and observe 9 polydactyl cats in this sample (i.e. \\(\\widehat{p}\\) = 9%).\nDoes this provide concrete evidence that the Quora claim is incorrect? Not really!\nBut, say our sample of 100 cats contains 80 polydactyl cats (\\(\\widehat{p}\\) = 80%). Or, say we saw only 1 polydactyl cat in a sample of 100 (\\(\\widehat{p}\\) = 1%).\nNow, it is possible that the Quora claim is true and we just happened to get extraordinarily lucky (or unlucky).\nBut, it’s probably more likely that we should start to question the validity of the Quora statistic."
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#hypothesis-testing-1",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#hypothesis-testing-1",
    "title": "Final Exam Review",
    "section": " Hypothesis Testing",
    "text": "Hypothesis Testing\nGeneral Framework\n\nSo where’s the cutoff - how many polydactyl cats do we need to observe in a sample of n before we start to question the Quora statistic?\nThis is the general framework of hypothesis testing.\nWe start off with a pair of competing claims, called the null hypothesis and the alternative hypothesis.\n\nThe null hypothesis is usually set to be the “status quo”. For instance, in our polydactyly example, we would set the null hypothesis (denoted H0, and read “H-naught”) to be “10% of cats are polydactyl.”\n\nFor the purposes of this class, the null hypothesis is always a statement of equality: \\(H_0: \\ \\theta = \\theta_0\\) for some null value \\(\\theta_0\\)."
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#hypothesis-testing-2",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#hypothesis-testing-2",
    "title": "Final Exam Review",
    "section": " Hypothesis Testing",
    "text": "Hypothesis Testing\nGeneral Framework\n\nGiven a null \\(H_0: \\ \\theta = \\theta_0\\), three possible alternative hypotheses present themselves to us (among which we must pick one):\n\n\\(H_1: \\ \\theta &lt; \\theta_0\\) (lower-tailed)\n\\(H_1: \\ \\theta &gt; \\theta_0\\) (upper-tailed)\n\\(H_1: \\ \\theta \\neq \\theta_0\\) (two-tailed)\n\n\n\n\n\n\n\n\n\nImportant\n\n\nThere should be NO OVERLAP between the null and alternative.\n\n\n\n\n\nFor example, it is incorrect to write a lower-tailed alternative as \\(H_1: \\ \\theta \\leq \\theta_0\\). Can anyone tell me why, conceptually, this is?"
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#states-of-the-world",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#states-of-the-world",
    "title": "Final Exam Review",
    "section": " States of the World",
    "text": "States of the World\n\nIn a given hypothesis testing setting, the null is either true or not (though we won’t ever get to know for sure).\nIndependently, our test will either reject the null or not.\nThis leads to four states of the world:\n\n\n\n\n\n\n\n\nResult of Test\n\n\n\n\n\n\n\nReject\n\n\nFail to Reject\n\n\n\n\nH0\n\n\nTrue\n\n\n\n\n\n\n\n\nFalse\n\n\n\n\n\n\n\n\n\nSome of these states are good, others are bad. Which are which?"
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#states-of-the-world-1",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#states-of-the-world-1",
    "title": "Final Exam Review",
    "section": " States of the World",
    "text": "States of the World\n\n\n\n\n\n\nResult of Test\n\n\n\n\n\n\n\nReject\n\n\nFail to Reject\n\n\n\n\nH0\n\n\nTrue\n\n\nBAD\n\n\nGOOD\n\n\n\n\nFalse\n\n\nGOOD\n\n\nBAD\n\n\n\n\nWe give names to the two “bad” situations: Type I and Type II errors.\n\n\n\n\n\n\n\n\nResult of Test\n\n\n\n\n\n\n\nReject\n\n\nFail to Reject\n\n\n\n\nH0\n\n\nTrue\n\n\nType I Error\n\n\nGOOD\n\n\n\n\nFalse\n\n\nGOOD\n\n\nType II Error\n\n\n\n\n\n\n\n\n\n\n\nDefinition: Type I and Type II errors\n\n\n\n\nA Type I Error occurs when we reject \\(H_0\\), when \\(H_0\\) was actually true.\nA Type II Error occurs when we fail to reject \\(H_0\\), when \\(H_0\\) was actually false."
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#states-of-the-world-2",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#states-of-the-world-2",
    "title": "Final Exam Review",
    "section": " States of the World",
    "text": "States of the World\nLevel and Power\n\nThe level of significance (aka “significance level”; aka “level”) of a test, denoted by \\(\\alpha\\), is defined to be the probability of committing a Type I error.\nThe power of a test, often denoted \\(Q(\\theta')\\), is \\[Q(\\theta') := \\mathbb{P}(\\text{Reject $H_0$, when the true value of $\\theta$ was $\\theta'$}) \\]\nGenerally, we fix the level and try and find the test with the most power (or, equivalently, with the smallest probability of committing a Type II error).\n\nThis leads us to the notion of a Most Powerful Test of Level α (from Topic 15)."
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#example-1-1",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#example-1-1",
    "title": "Final Exam Review",
    "section": " Example 1",
    "text": "Example 1\n\n\n\n\n\n\nExample 1\n\n\nLet \\(Y_1, \\cdots, Y_n \\iid \\mathcal{N}(\\mu, 1)\\) for some unknown µ, and suppose we wish to test H0: µ = µ0 vs HA: µ &gt; µ0 at a 0.05 level of significance. We propose two tests:\n\nTest 1: Reject H0 when \\(Y_1 - \\mu_0 &gt; \\Phi^{-1}(0.95)\\)\nTest 2: Reject H0 when \\(\\frac{\\overline{Y}_n - \\mu_0}{1/\\sqrt{n}} &gt; \\Phi^{-1}(0.95)\\)\n\n\nVerify that both tests have a 5% level of significance.\nDerive expressions for the power functions of both tests."
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#example-1-3",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#example-1-3",
    "title": "Final Exam Review",
    "section": " Example 1",
    "text": "Example 1\nPart (a)\n\nLet’s focus on Test 1.\nBy definition, the level of the test is the probability of rejecting the null when the null was true.\nSaying that “the null was true” is saying that the true value of µ is µ0, in which case \\(Y_1 \\sim \\mathcal{N}(\\mu_0, 1)\\).\nHence, the probability of rejecting the null (i.e. that \\(Y_1 - \\mu_0 &gt; \\Phi^{-1}(0.95)\\)) if the null is true is:\n\n\n\\[\\begin{align*}\n  \\Prob_{H_0}(Y_1 - \\mu_0 &gt; \\Phi^{-1}(0.95)) & = 1 - \\Prob_{H_0}(Y_1 - \\mu_0 \\leq \\Phi^{-1}(0.95)) \\\\\n    & = 1 - \\Phi[\\Phi^{-1}(0.95)] = 1 - 0.95 = 0.05 \\ \\checkmark\n\\end{align*}\\]\n\n\nTry Test 2 on your own."
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#example-1-4",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#example-1-4",
    "title": "Final Exam Review",
    "section": " Example 1",
    "text": "Example 1\nPart (b)\n\nWe now turn our attention to the power curves. Again, we start with Test 1.\nQ(µA) is the probability of rejecting the null when the true value of µ is in fact µA.\nSaying that “the true value of µ is in fact µA” means \\(Y_1 \\sim \\mathcal{N}(\\mu_A, 1)\\). Furthermore, we reject the null when \\(Y_1 &gt; \\Phi^{-1}(0.95)\\).\nHence,"
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#example-3-3",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#example-3-3",
    "title": "Final Exam Review",
    "section": " Example 3",
    "text": "Example 3\nPart (b)\n\n\\[\\begin{align*}\n  Q_1(\\mu_A) & = \\Prob_{\\mu_A}(Y_1 - \\mu_0 &gt; \\Phi^{-1}(0.95)) \\\\\n  &  = \\Prob_{\\mu_A}(Y_1 - {\\color{blue} \\mu_A + \\mu_A} -  \\mu_0 &gt; \\Phi^{-1}(0.95)) \\\\\n   &  = \\Prob_{\\mu_A}({\\color{red}Y_1 - \\mu_A} &gt; \\Phi^{-1}(0.95) + (\\mu_0 - \\mu_A)) \\\\\n   &  = 1 - \\Phi[\\Phi^{-1}(0.95) + (\\mu_0 - \\mu_A)]\n\\end{align*}\\]\n\n\nFor test 2:\n\n\n\\[\\begin{align*}\n  Q_2(\\mu_A) & = \\Prob_{\\mu_A}\\left(\\frac{\\overline{Y}_n - {\\color{blue} \\mu_A + \\mu_A} - \\mu_0}{1/\\sqrt{n}} &gt; \\Phi^{-1}(0.95) \\right) \\\\\n  & = \\cdots = 1 - \\Phi\\left[\\Phi^{-1}(0.95) + \\sqrt{n}(\\mu_0 - \\mu_A) \\right]\n\\end{align*}\\]"
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#example-3-4",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#example-3-4",
    "title": "Final Exam Review",
    "section": " Example 3",
    "text": "Example 3\nPart (b)"
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#hypothesis-testing-3",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#hypothesis-testing-3",
    "title": "Final Exam Review",
    "section": " Hypothesis Testing",
    "text": "Hypothesis Testing\np-Value Framework\n\nInstead of the critical value framework, we can also conduct hypothesis tests using p-values\nThe p-value is the probability of observing something as or more extreme (in the direction of the alternative) than what we actually observed.\n\n\n\n\nLower-tailed: ℙ(TS &lt; ts)\nUpper-tailed: ℙ(TS &gt; ts)\nTwo-sided: ℙ(|TS| &gt; ts)\nA picture is worth a thousand words!"
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#hypothesis-testing-4",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#hypothesis-testing-4",
    "title": "Final Exam Review",
    "section": " Hypothesis Testing",
    "text": "Hypothesis Testing\nPSTAT 120B\n\nIn PSTAT 120B, we covered:\n\nLarge- and small-sample tests for the mean\nSmall-sample tests for a difference in means\nTests for the variance (assuming a normal population)\n\nMake sure that, for each, you understand:\n\nWhat assumptions are required\nHow to conduct them (in both the critical value and p-value frameworks)\n\nKeep in mind, hypothesis test questions on the final exam for PSTAT 120B are often (not always, though) word problems!"
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#hypothesis-testing-5",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#hypothesis-testing-5",
    "title": "Final Exam Review",
    "section": " Hypothesis Testing",
    "text": "Hypothesis Testing\nPSTAT 120B\n\nI find it useful to also quickly review how these tests are derived. This can, in my opinion, help with the memorization aspect.\nFor example, suppose we are testing \\(H_0: \\mu = \\mu_0\\) against \\(H_A: \\mu \\neq \\mu_0\\) using data \\(Y_1, \\cdots, Y_n \\iid \\mathcal{N}(\\mu, 1)\\).\n\nA natural point estimator for \\(\\mu\\) is \\(\\bar{Y}\\), which we know is normally-distributed, so a natural test statistic is its standardized form, under the null: \\(Z := (\\bar{Y} - \\mu_0)/(\\sigma / \\sqrt{n})\\).\nIf \\(\\bar{Y}\\) is far from \\(\\mu_0\\) (equivalently, that \\(Z\\) is far from zero), we have evidence that the true mean is not \\(\\mu_0\\): i.e. we have evidence against the null and in favor of the alternative.\nThis reveals a rejection region of the form \\(|Z| &gt; c\\) for some critical value \\(c\\), which can be derived by setting the level of the test to be \\(\\alpha\\)."
  },
  {
    "objectID": "Hidden/120B-F25/FinRev/FinRev_v3.html#hypothesis-testing-6",
    "href": "Hidden/120B-F25/FinRev/FinRev_v3.html#hypothesis-testing-6",
    "title": "Final Exam Review",
    "section": " Hypothesis Testing",
    "text": "Hypothesis Testing\nPSTAT 120B\n\nTake a look through 10.7 of the textbook, titled “Some Comments on the Theory of Hypothesis Testing.” The authors provide some (in my opinion) very useful and practical comments on hypothesis testing.\nAlso, even though material from Topic 15 will not feature heavily on the exam (if at all - again, I haven’t seen the exam yet!) I HIGHLY recommend you take a look at it before moving on to your future statistics courses.\n\nPretty much every course will at least in part make reference to that material, even if behind the scenes.\n\nAny time you perform a hypothesis test (e.g. in: Machine Learning, Time Series, etc.), you’re often concerned with finding a test with optimal power. The Neyman-Pearson Lemma and Likelihood Ratio Tests gives you such a test in many cases!"
  }
]