---
title: "Week 1 Discussion Section"
subtitle: "PSTAT 120B, Spring 2025, with Dr. Brian Wainwright"
footer: "PSTAT 120B Sp25; Discussion Section 1, © Ethan P. Marzban"
logo: "Images/logo.png"
format: 
  clean-revealjs:
    theme: slides.scss
    multiplex: true
    transition: fade
    slide-number: true
    incremental: true 
    chalkboard: true
    include-before: [ '<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {enableAssistiveMml: false}});</script>']
    menu:
      side: left
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
author:
  - name: Ethan P. Marzban
    affiliations: Department of Statistics and Applied Probability; UCSB <br /> <br />
institute: "April 1, 2025"
title-slide-attributes:
    data-background-image: "Images/logo.png"
    data-background-size: "35%"
    data-background-opacity: "0.5"
    data-background-position: 80% 50%
code-annotations: hover
---

::: hidden
$$
\newcommand\R{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\1}{1\!\!1}
\newcommand{\comp}[1]{#1^{\complement}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\SD}{\mathrm{SD}}
\newcommand{\vect}[1]{\vec{\boldsymbol{#1}}}
\newcommand{\Cov}{\mathrm{Cov}}
\usepackage[makeroom]{cancel}
$$
:::

```{css echo = F}
.hscroll {
  height: 100%;
  max-height: 600px;
  max-width: 2000px;
  overflow: scroll;
}

.hscroll2 {
  height: 100%;
  max-height: 300px;
  overflow: scroll;
}

.hscroll3 {
  max-width: 2rem;
  overflow: scroll;
}
```

```{r setup, echo = F}
library(tidyverse)
library(countdown)
library(fixest)
library(modelsummary) # Make sure you have >=v2.0.0
library(GGally)
library(ggokabeito)
library(reshape2)
library(pander)
library(gridExtra)
library(cowplot)
library(readxl)      # for reading in Excel files
library(plotly)
```

## {{< fa rocket >}} Probability Spaces

-   [**Probability Space:**]{.alert} $(\Omega, \F, \Prob)$
    -   [**Outcome Space:**]{.alert} $\Omega$; enumeration of all possible outcomes of an experiment
    
-   [**Random Variable:**]{.alert} mapping from $\Omega$ to $\R$.
    -   We typically use capital letters later in the alphabet (e.g. _X_, _Y_, _Z_) to denote
random variables
    -   [**Support**]{.alert}: $X(\Omega)$; the set of all possible values mapped to
    -   Finite or countable support $\implies$ [**discrete**]{.alert}; uncountable $\implies$ [**continuous**]{.alert}

## {{< fa dice >}} Random Variables
### Discrete Case

-   [**Probability Mass Function**]{.alert} (PMF): $p_X(k) := \Prob(X = k)$
    -   Probabilities are found by summing the PMF: $\Prob(X \in B) = \sum_{k \in B}p_X(k)$

-   [**Cumulative Mass Function**]{.alert} (CMF): $F_X(x) := \Prob(X \leq x)$, calculated as $F_X(x) = \sum_{k \leq x} p_X(k)$

-   [**Expectation**]{.alert}: $\E[X] := \sum_{k} k p_X(k)$
    -   A measure of the "center" of the distribution
    
-   [**LOTUS**]{.alert} (Law of the Unconscious Statistician): $\E[g(X)] := \sum_{k} g(k) p_X(k)$

-   [**Variance**]{.alert}: $\Var(X) := \E\left[ (X - \E[X])^2 \right]  = \E[X^2] - (\E[X])^2$
    -   A measure of the “spread” of the distribution
    
    

## {{< fa dice >}} Random Variables
### Continuous Case

-   [**Probability Density Function**]{.alert} (PDF): $f_X(x)$ such that $\Prob(X \in B) = \int_{B} f_X(x) \ \mathrm{d}x$
    -   Any function that is [nonnegative]{.underline} and [integrates to unity]{.underline} is a valid PDF.

-   [**Cumulative Distribution Function**]{.alert} (CDF): $F_X(x) := \Prob(X \leq x)$, calculated as $F_X(x) = \int_{-\infty}^{x} f_X(t) \ \mathrm{d}t$

-   [**Expectation**]{.alert}: $\E[X] := \int_{\R} x f_X(x) \ \mathrm{d}x$
    
-   [**LOTUS**]{.alert} (Law of the Unconscious Statistician): $\E[g(X)] := \int_{\R} g(x) f_X(x) \ \mathrm{d}x$

-   [**Variance**]{.alert}: $\Var(X) := \E\left[ (X - \E[X])^2 \right]  = \E[X^2] - (\E[X])^2$

## {{< fa bax-open >}} Moment-Generating Function

::: {.callout-note}
## **Definition:** Moment-Generating Function
The [**moment-generating function**]{.alert} (MGF) of a random variable _X_ is defined as
$$ M_X(t) := \E[e^{tX}] $$
:::

-   Why the name? $M_X^{(n)}(0) = \E[X^n]$.

-   Note: for every random variable, $M_X(0) = 1$. But, it is not always the case that the MGF is _continuous_ at or near zero.
    -   If it is continuous in a small neighborhood including zero, it uniquely determines a distribution.
    
    
## {{< fa pencil >}} Exercise 1
### The Gamma Distribution

::: {.callout-tip}
## **Exercise 1**

A random variable _X_ is said to follow the [**Gamma Distribution**]{.alert} with parameters $\alpha$ and $\beta$, notated $X \sim \mathrm{Gamma}(\alpha, \beta)$, if it has density given by
$$ f_X(x) = \begin{cases} c \cdot x^{\alpha - 1} e^{-x / \beta} & \text{if } x \geq 0 \\ 0 & \text{otherwise} \\ \end{cases} $$
(Note that this might be slightly different than the version of the Gamma distribution you saw in PSTAT 120A.)

::: {.nonincremental}
a)  Find the value of the constant _c_.
b)  Find the MGF of _X_, and use this to compute $\E[X]$.
:::

:::

## {{< fa clock >}} The Gamma Distribution

\

```{ojs}
viewof alph = Inputs.range(
  [0.2, 10], 
  {value: 2, step: 0.1, label: "α:"}
)

viewof bet = Inputs.range(
  [0.2, 3.1], 
  {value: 1, step: 0.01, label: "β:"}
)
```

```{ojs}
jstat = require("jstat")

plt_pdf = Plot.plot({
    width: 700,
    height: 300,
    color: {
      legend: true
    },
    x: {
      label: "x",
      axis: true
    },
    y: {
      label: "f(x)",
      //axis: false,
      //domain: [0, d3.max(pdfvals.map(d => d.pdf))]  
    },
    marks: [
      Plot.ruleY([0]),
      Plot.ruleX([0]),
      Plot.line(pdfvals, {x: "x", y: "pdf", stroke : "blue", strokeWidth: 4})
    ]
  })
  
pdfvals = {
  const x = d3.range(0, 12, 0.01);
  var pdf;
  pdf = x.map(x => ({x: x, pdf: jstat.gamma.pdf(x, alph, bet)}));
  return pdf
}
```


## {{< fa clock >}} The Gamma Distribution
### Special Cases

-   If $X \sim \mathrm{Gamma}(\nu / 2, \ 2)$ for some $\nu > 0$, we say _X_ follows the [**chi-square distribution**]{.alert} with $\nu$ [**degrees of freedom**]{.alert}. 
    -   For example, the $\mathrm{Gamma}(3, 2)$ distribution is equivalent to the $\chi^2_{6}$ distribution.

-   If $X \sim \mathrm{Gamma}(1, \beta)$ for some $\beta > 0$, then $X \sim \mathrm{Exp}(\beta)$.


\

-   The Gamma distribution also turns out to have some incredibly nice properties, making it a very important distribution to know.
    -   Said differently, expect to see the Gamma distribution arise again in this course!
    
## {{< fa bell >}} The Normal Distribution


::: {.callout-note}
## **Definition: Normal Distribution**

A random variable _X_ is said to follow the normal distribution with parameters $\mu$ and $\sigma^2$, notated $X \sim \mathcal{N}(\mu, \sigma^2)$, if _X_ has density
$$ f_X(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left\{ - \frac{1}{2 \sigma^2} (x - \mu)^2 \right\} $$
:::

-   The [**standard normal distribution**]{.alert} is a special case of the normal distribution where $\mu = 0$ and $\sigma^2 = 1$. Its PDF and CDF are denoted $\phi(z)$ and $\Phi(z)$, respectively:
$$ \phi(z) = \frac{1}{\sqrt{2\pi}} e^{-z^2 / 2} \qquad \Phi(z) := \int_{-\infty}^{z} \frac{1}{\sqrt{2\pi}} e^{-t^2 / 2} \ \mathrm{d}t $$



## {{< fa bell >}} The Normal Distribution

\

```{ojs}
viewof mu = Inputs.range(
  [-3, 3], 
  {value: 0, step: 0.1, label: "µ:"}
)

viewof sig2 = Inputs.range(
  [0.1, 8], 
  {value: 1, step: 0.01, label: "σ²"}
)
```

```{ojs}
plt_pdf2 = Plot.plot({
    width: 700,
    height: 300,
    color: {
      legend: true
    },
    x: {
      label: "x",
      axis: true
    },
    y: {
      label: "f(x)",
      //axis: false,
      //domain: [0, d3.max(pdfvals.map(d => d.pdf))]  
    },
    marks: [
      Plot.ruleY([0]),
      Plot.ruleX([0]),
      Plot.line(pdfvals2, {x: "x", y: "pdf", stroke : "blue", strokeWidth: 4})
    ]
  })
  
pdfvals2 = {
  const x = d3.range(-6, 6, 0.01);
  var pdf;
  pdf = x.map(x => ({x: x, pdf: jstat.normal.pdf(x, mu, Math.sqrt(sig2))}));
  return pdf
}
```


 
## {{< fa pencil >}} Exercise 2
### Standard Normal MGF

::: {.callout-tip}
## **Exercise 1**

Find the MGF of the standard normal distribution.

:::

 
## {{< fa note-sticky >}} A Note

-   I'd like to close off by imparting (what I find to be) a useful "trick" / observation to y'all.

::: {.fragment}
::: {.callout-tip}
## **Tip**
A distribution is uniquely determined by the _variable_ part of its PDF.
:::
:::

-   For example, if I tell you _X_ has density $f_X(x) = c \cdot e^{-2x^2}$, we can immediately say that $X \sim \mathcal{N}(0, 1/4)$ distribution, because this is the _only_ distribution whose PDF has variable portion $e^{-2x^2}$.
    -   This is a direct consequence of the fact that densities must integrate to unity!
    
-   Try this on for size: if _Y_ has PDF $f_Y(y) = c \cdot y^3 e^{-y/2}$ for _y_ > 0, what distribution does _Y_ follow?