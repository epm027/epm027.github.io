---
title: "Week 6 Discussion Section"
subtitle: "PSTAT 120B, Spring 2025, with Dr. Brian Wainwright"
footer: "PSTAT 120B Sp25; Discussion Section 6, © Ethan P. Marzban"
logo: "Images/logo.png"
format: 
  clean-revealjs:
    theme: slides.scss
    multiplex: true
    transition: fade
    slide-number: true
    incremental: true 
    chalkboard: true
    include-before: [ '<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {enableAssistiveMml: false}});</script>']
    menu:
      side: left
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
author:
  - name: Ethan P. Marzban
    affiliations: Department of Statistics and Applied Probability; UCSB <br /> <br />
institute: "May 6, 2025"
title-slide-attributes:
    data-background-image: "Images/logo.png"
    data-background-size: "35%"
    data-background-opacity: "0.5"
    data-background-position: 80% 50%
code-annotations: hover
---

::: hidden
$$
\newcommand\R{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\1}{1\!\!1}
\newcommand{\comp}[1]{#1^{\complement}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\SD}{\mathrm{SD}}
\newcommand{\vect}[1]{\vec{\boldsymbol{#1}}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Corr}{\mathrm{Corr}}
\usepackage[makeroom]{cancel}
\newcommand{\probto}{\stackrel{\mathrm{p}}{\longrightarrow}}
$$
:::

```{css echo = F}
.hscroll {
  height: 100%;
  max-height: 600px;
  max-width: 2000px;
  overflow: scroll;
}

.hscroll2 {
  height: 100%;
  max-height: 300px;
  overflow: scroll;
}

.hscroll3 {
  max-width: 2rem;
  overflow: scroll;
}
```

```{r setup, echo = F}
library(tidyverse)
library(countdown)
library(fixest)
library(modelsummary) # Make sure you have >=v2.0.0
library(GGally)
library(ggokabeito)
library(reshape2)
library(pander)
library(gridExtra)
library(cowplot)
library(readxl)      # for reading in Excel files
library(plotly)
```


## {{< fa bullhorn >}} Announcements

-   [**Midterm Exam:**]{.bg style="--col: #f5e287"} TOMORROW (May 7) during Lecture, in the Lecture Hall (BUCHN 1940)

-   Homework 3 Revisions due **this Sunday** (May 11, 2025)
    -   Same instructions as before
    -   Though this isn't strictly required, it will help if you "match" your first page to the corresponding Gradescope question
    
-   Homework 4 Initial Submission is **delayed** until **next Sunday** (May 18, 2025) at 11:59pm
    -   Will cover topics like the Cramér-Rao Lower Bound, Sufficiency, Efficiency, etc.


## {{< fa backward-fast >}} Recap
### Framework for Estimation

:::: {.columns}

::: {.column width="50%"}
![](Images/Inference.svg)
:::

::: {.column width="50%"}
-   **Goal:** to estimate a population parameter (aka [**estimand**]{.alert})

-   To do so, we take random [**samples**]{.alert} from the population.

-   From these samples we construct [**statistics**]{.alert}, which are [**estimators**]{.alert} if they are used in service of estimating an estimand.
    -   The corresponding estimator derived from a _realized_ sample $\vect{x}$ is called an [**estimate**]{.alert} of the estimand.
:::

::::


## {{< fa backward-fast >}} Recap
### Properties of Estimators

-   [**Mean Squared-Error**]{.alert} (MSE): $\mathrm{MSE}(\widehat{\theta}_n, \theta) := \E[(\widehat{\theta} - \theta)^2]$
    -   [**Bias-Variance Decomposition**]{.alert}: $\mathrm{MSE}(\widehat{\theta}_n, \theta)  = \mathrm{Bias}^2(\widehat{\theta}_n, \theta) + \Var(\widehat{\theta}_n)$
    -   An "ideal" estimator is [**unbiased**]{.alert} (i.e. has expectation equal to the estimand) and has _low_ variance; i.e. has _low_ MSE.
    
::: {.fragment style="text-align: center"}
![](Images/mark1.svg){width="20%"} \ \ \ \ \  ![](Images/mark2.svg){width="20%"}
:::




## {{< fa arrow-right >}} Consistency
### Another Property

::: {.callout-note}
## **Definition:** Consistency

We say an estimator $\widehat{\theta}_n$ is [**consistent**]{.alert} for an estimand $\theta$, notated $\widehat{\theta}_n \probto \theta$, if, for every $\varepsilon > 0$,
$$ \lim_{n \to \infty} \Prob(|\widehat{\theta}_n - \theta| \leq \varepsilon) = 1 $$
which is equivalent to
$$ \lim_{n \to \infty} \Prob(|\widehat{\theta}_n - \theta| > \varepsilon) = 0 $$
:::

-   "$\widehat{\theta}_n$ converges in probability to $\theta$"


## {{< fa arrow-right >}} Consistency
### Understanding the Definition

-   $|\widehat{\theta} - \theta|$ can be thought of as the "distance" between $\widehat{\theta}_n$ and $\theta$.

-   Therefore, the even $\{|\widehat{\theta} - \theta| \leq \varepsilon\}$ is: "the distance between $\widehat{\theta}_n$ and $\theta$ is small"
    -   Equivalently: "$\widehat{\theta}_n$ and $\theta$ are very close to each other".
    
-   Therefore, $\Prob(|\widehat{\theta}_n - \theta| \leq \varepsilon)$ is "the probability that "$\widehat{\theta}_n$ and $\theta$ are very close to each other"

-   Consistency asserts this probability goes to 1 as $n \to \infty$
    -   I.e. "as the sample size increases, we become more and more certain that $\widehat{\theta}_n$ and $\theta$ are very close to each other."
    
    
## {{< fa arrow-right >}} Consistency
### A Useful Theorem

::: {.callout-important}
## **Theorem 9.1**

If $\widehat{\theta}_n$ is an unbiased estimator for $\theta$, then $\widehat{\theta}_n \probto \theta$ if $\lim\limits_{n \to \infty} \Var(\widehat{\theta}_n) = 0$.
:::

-   **Caution**: we can only use this theorem if our estimator is unbiased.

-   Let's prove this theorem on the board.

## {{< fa arrow-right >}} Consistency
### Two Useful Results

::: {.callout-important}
## **Weak Law of Large Numbers** (WLLN)
If $Y_1, \cdots, Y_n$ is an i.i.d. sample from a population with mean $\mu$ and finite variance $\sigma^2$, then the sample mean is a consistent estimator for $\mu$.
:::

::: {.fragment}

::: {.callout-important}
## **Continuous Mapping Theorem** (CMT)
If $\widehat{\theta}_n \probto \theta$ and $g(\cdot)$ is a continuous function (over the support of $\widehat{\theta}_n$), then $g(\widehat{\theta_n}) \probto g(\theta)$.
:::

:::

-   In many cases, these two theorems can be used to establish consistency without having to appeal to the definition of consistency.



## {{< fa arrow-right >}} Consistency
### Weak Law of Large Numbers

```{r}
set.seed(123)

B <- 1000
K <- 50
p <- 0.25

X <- matrix(rep(NA, B * K), nrow = B)

for(k in 1:K){
  temp_samp <- sample(c(0, 1), size = B, replace = T, prob = c(1 - p, p))
  X[,k] <- cumsum(temp_samp) / (1:B)
}

X <- data.frame(X)

X %>% 
  melt(
    variable.name = "samp",
    value_name = "value"
  ) %>% mutate(
    x = rep(1:B, K)
    ) %>% ggplot(aes(x = x, y = value)) +
  geom_line(aes(group = samp), alpha = 0.3, linewidth = 0.8) +
  theme_minimal(base_size = 16) + 
  xlab("num. trials") + ylab("Est. of p") + 
  ggtitle("Estimating a Proportion") +
  geom_hline(yintercept = p, col = "red", linewidth = 1)
```




## {{< fa arrow-right >}} Consistency
### Sample Standard Deviation

```{r}
set.seed(123)


B <- 1000
K <- 50

sd_mat <- matrix(rep(NA, B * K), nrow = B)

for(k in 1:K){
  temp_samp <- rnorm(B, 0, 1.5)
  for(b in 1:B){
    sd_mat[b,k] <- sd(temp_samp[1:b])
  }
}

sd_mat <- data.frame(sd_mat)

sd_mat %>% 
  melt(
    variable.name = "samp",
    value_name = "value"
  ) %>% mutate(
    x = rep(1:B, K)
  ) %>% ggplot(aes(x = x, y = value)) +
  geom_line(aes(group = samp), alpha = 0.3, linewidth = 0.8) +
  theme_minimal(base_size = 16) +
  ggtitle("Estimating an S.D.") +
  xlab("samp. size") + ylab("samp. sd") +
  geom_hline(yintercept = 1.5, col = "red", linewidth = 1)
```


## {{< fa arrow-right >}} Consistency
### Consistency and Unbiasedness

-   Note; there is some overlap between consistency and unbiasedness, but there is also some non-overlap.

-   For example, $\widehat{\sigma}_n^2 := \frac{1}{n} \sum_{i=1}^{n} (Y_i - \overline{Y}_n)^2$ is a consistent estimator for the population variance $\sigma^2$ (we'll show this on today's worksheet). 
    -   But, is it biased?

-   Additionally, can anyone give me an example of an unbiased estimator that is _not_ consistent?



## {{< fa note-sticky >}} General Notes

-   As you may have noticed, things are starting to get a bit more mathematical.
    -   Specifically, note that we have been relying more and more on _theorems_.
    
-   Here are some practical tips:
    1)    If you are going to use a theorem, **check that the relevant conditions/assumptions hold**. If they do not, then you cannot use the theorem.
    2)    **Cite which theorem you are using**. This can be as simple as a note in the margin ("e.g. by the Central Limit Theorem...").
